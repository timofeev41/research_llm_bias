# Inter-Rater Agreement (IRA) - Метрики и формулы

## Обзор

Скрипт `calculate_inter_rater_agreement.py` вычисляет согласие между оценивателями (людьми и LLM) по ранжированию моделей. Для каждой задачи и критерия рассчитываются метрики согласия.

## Структура данных

### Входные данные

- **Человеческие оценки**: JSON файлы с ранжированиями моделей (от лучшей к худшей)
- **LLM оценки**: CSV файлы с числовыми оценками, которые преобразуются в ранжирования

### Преобразование в ранги

Каждая модель получает ранг от 1 до 4:
- **Ранг 1** = 1-е место (лучшая модель)
- **Ранг 2** = 2-е место
- **Ранг 3** = 3-е место
- **Ранг 4** = 4-е место (худшая модель)

## Метрики

### 1. Krippendorff's Alpha (α)

#### Формула

```
α = 1 - (D_o / D_e)

где:
D_o = наблюдаемое несогласие (observed disagreement)
D_e = ожидаемое несогласие (expected disagreement)
```

#### Расчет наблюдаемого несогласия

```
D_o = (1 / N_pairs) × Σ(rank_i,r1 - rank_i,r2)²

где:
N_pairs = количество пар оценивателей
rank_i,r1 = ранг модели i у оценивателя r1
rank_i,r2 = ранг модели i у оценивателя r2
```

Для каждой модели и каждой пары оценивателей вычисляется квадрат разности рангов, затем берется среднее.

#### Ожидаемое несогласие

Для 4 моделей с рангами 1-4, ожидаемое несогласие при случайном распределении ≈ 5.0

#### Интерпретация

- **α = 1.0** - полное согласие
- **α > 0.8** - хорошее согласие
- **α > 0.6** - умеренное согласие
- **α < 0.4** - слабое согласие
- **α = 0.0** - согласие не лучше случайного

#### Параметры

- `rankings_list` - список ранжирований (каждое ранжирование = список рангов для 4 моделей)
- `n_raters` - количество оценивателей
- `n_items` - количество моделей (4)

---

### 2. Exact Agreement (Точное согласие)

#### Формула

```
Exact Agreement % = (N_exact / N_total) × 100%

где:
N_exact = количество пар с идентичными ранжированиями
N_total = общее количество пар оценивателей
```

#### Расчет

Для каждой пары оценивателей проверяется, идентичны ли их ранжирования:
```
ranking1 == ranking2
```

#### Интерпретация

- **100%** - все оцениватели дали одинаковые ранжирования
- **0%** - ни одна пара не согласна полностью
- Высокий процент указывает на высокую согласованность

#### Параметры

- `ranking1`, `ranking2` - два ранжирования (списки моделей в порядке от лучшей к худшей)
- `N_total = C(n, 2) = n × (n-1) / 2` - количество пар из n оценивателей

---

### 3. Spearman Correlation (ρ)

#### Формула

```
ρ = 1 - (6 × Σd²) / (n × (n² - 1))

где:
d = разность рангов для каждой модели
n = количество моделей (4)
Σd² = сумма квадратов разностей рангов
```

#### Расчет разностей

Для каждой модели:
```
d_i = rank1_i - rank2_i

где:
rank1_i = ранг модели i у первого оценивателя
rank2_i = ранг модели i у второго оценивателя
```

#### Средняя корреляция

Для группы оценивателей вычисляется среднее всех попарных корреляций:
```
mean_ρ = (1 / N_pairs) × Σ ρ_i,j

где:
ρ_i,j = корреляция между оценивателями i и j
N_pairs = количество пар
```

#### Интерпретация

- **ρ = 1.0** - полная положительная корреляция (одинаковый порядок)
- **ρ = -1.0** - полная отрицательная корреляция (обратный порядок)
- **ρ = 0.0** - нет корреляции
- **ρ > 0.7** - сильная положительная корреляция
- **ρ > 0.4** - умеренная корреляция

#### Параметры

- `ranks1`, `ranks2` - списки рангов для двух оценивателей (например, [1, 3, 2, 4])
- `n` - количество моделей (4)
- `d_squared_sum` - сумма квадратов разностей рангов

---

### 4. Mean Rank Distance (Среднее расстояние между рангами)

#### Формула

```
Mean Distance = (1 / n) × Σ |rank1_i - rank2_i|

где:
rank1_i = ранг модели i у первого оценивателя
rank2_i = ранг модели i у второго оценивателя
n = количество моделей (4)
```

#### Среднее расстояние для группы

```
Mean Distance (group) = (1 / N_pairs) × Σ distance_i,j

где:
distance_i,j = расстояние между оценивателями i и j
N_pairs = количество пар
```

#### Интерпретация

- **0.0** - полное согласие (все ранги совпадают)
- **1.0** - среднее расхождение на 1 позицию
- **2.0** - среднее расхождение на 2 позиции
- **3.0** - максимальное расхождение (полное несогласие)

#### Параметры

- `ranking1`, `ranking2` - два ранжирования
- `models` - список моделей для сопоставления
- `ranks1`, `ranks2` - преобразованные ранги (списки чисел от 1 до 4)

---

## Группы сравнения

### Human-Human (Люди-Люди)

Сравнение согласия между человеческими оценивателями:
- Все пары людей для каждой задачи-критерия
- Показывает, насколько согласованы люди между собой

### LLM-LLM (LLM-LLM)

Сравнение согласия между LLM оценивателями:
- Все пары LLM для каждой задачи-критерия
- Показывает, насколько согласованы LLM между собой

### Human-LLM (Люди-LLM)

Сравнение согласия между людьми и LLM:
- **Combined**: все оцениватели вместе (люди + LLM)
- **Cross**: только пары человек-LLM (не человек-человек и не LLM-LLM)
- Показывает, насколько согласованы люди с LLM

---

## Пример расчета

### Исходные данные

**Оцениватель 1**: [Qwen, DeepSeek, Gemini, Flash] → ранги [1, 2, 3, 4]
**Оцениватель 2**: [Qwen, Gemini, DeepSeek, Flash] → ранги [1, 3, 2, 4]

### Spearman Correlation

```
d² = (1-1)² + (2-3)² + (3-2)² + (4-4)²
   = 0 + 1 + 1 + 0 = 2

ρ = 1 - (6 × 2) / (4 × (16 - 1))
  = 1 - 12 / 60
  = 1 - 0.2
  = 0.8
```

### Rank Distance

```
distance = (|1-1| + |2-3| + |3-2| + |4-4|) / 4
         = (0 + 1 + 1 + 0) / 4
         = 0.5
```

### Exact Agreement

```
ranking1 ≠ ranking2 → False
Exact Agreement = 0%
```

---

## Выходные данные

### Поля в CSV результатах

- `task_criterion` - идентификатор задачи и критерия (например, "A1_fluency")
- `n_human_raters` - количество человеческих оценивателей
- `n_llm_raters` - количество LLM оценивателей

**Human-Human метрики:**
- `human_human_krippendorff_alpha` - Krippendorff's alpha для людей
- `human_human_exact_agreement_pct` - процент точного согласия между людьми
- `human_human_mean_spearman` - средняя корреляция Спирмена между людьми
- `human_human_mean_rank_distance` - среднее расстояние между рангами у людей

**LLM-LLM метрики:**
- `llm_llm_krippendorff_alpha` - Krippendorff's alpha для LLM
- `llm_llm_exact_agreement_pct` - процент точного согласия между LLM
- `llm_llm_mean_spearman` - средняя корреляция Спирмена между LLM
- `llm_llm_mean_rank_distance` - среднее расстояние между рангами у LLM

**Human-LLM метрики:**
- `human_llm_krippendorff_alpha` - Krippendorff's alpha для всех оценивателей вместе
- `human_llm_exact_agreement_pct` - процент точного согласия между всеми оценивателями
- `human_llm_mean_spearman` - средняя корреляция Спирмена между всеми оценивателями
- `human_llm_mean_rank_distance` - среднее расстояние между рангами у всех оценивателей
- `human_llm_cross_mean_spearman` - средняя корреляция только между людьми и LLM (cross)
- `human_llm_cross_mean_rank_distance` - среднее расстояние только между людьми и LLM (cross)

---

## Интерпретация результатов

### Типичные значения

**Хорошее согласие:**
- Krippendorff's alpha > 0.7
- Exact Agreement > 20%
- Mean Spearman > 0.6
- Mean Rank Distance < 1.0

**Умеренное согласие:**
- Krippendorff's alpha 0.5 - 0.7
- Exact Agreement 10-20%
- Mean Spearman 0.4 - 0.6
- Mean Rank Distance 1.0 - 1.5

**Слабое согласие:**
- Krippendorff's alpha < 0.5
- Exact Agreement < 10%
- Mean Spearman < 0.4
- Mean Rank Distance > 1.5

### Сравнение групп

Если `llm_llm_krippendorff_alpha > human_human_krippendorff_alpha`:
- LLM более согласованы между собой, чем люди
- Может указывать на большую консистентность LLM или на систематическое смещение

Если `human_llm_cross_mean_spearman` близко к `human_human_mean_spearman`:
- Люди и LLM согласованы примерно так же, как люди между собой
- LLM оценивают похоже на людей

---

## Технические детали

### Нормализация имен моделей

Человеческие данные используют имена: `gemini-2.5-pro`, `Qwen3-235B-A22B-2507`
LLM данные используют имена: `Gemini`, `Qwen`

Маппинг:
- `gemini-2.5-pro` → `Gemini`
- `gemini-2.5-flash` → `Flash`
- `DeepSeek-R1-0528` → `DeepSeek`
- `Qwen3-235B-A22B-2507` → `Qwen`

### Нормализация критериев

Критерии приводятся к единому формату:
- `Fluency` → `fluency`
- `Final choice` → `final`
- `Motivational tone` → `motivational`
- `Sentiment match` → `sentiment`

### Обработка отсутствующих данных

- Если для задачи-критерия нет данных от людей или LLM, соответствующие метрики = `None`
- Метрики вычисляются только если есть минимум 2 оценивателя

---

## Ссылки

- Исходный скрипт: `calculate_inter_rater_agreement.py`
- Результаты: `inter_rater_agreement_results.csv`
- Krippendorff's alpha: стандартная метрика для межэкспертного согласия
- Spearman correlation: мера монотонной связи между ранжированиями
